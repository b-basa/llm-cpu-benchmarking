# llm-vlm-cpu-benchmark
Benchmarking LLM on CPU, interested in tkn/s and memory usage.
Get llama.cpp from -> https://github.com/ggml-org/llama.cpp/releases
